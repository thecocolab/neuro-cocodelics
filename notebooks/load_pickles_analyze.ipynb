{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ” Analyzing: ../results/lsd_closed_hamza/lsd_closed_hamza_binary_baseline_rs42.pkl\n",
            "============================================================\n",
            "âœ… Successfully loaded pickle file\n",
            "ğŸ“Š Root object type: <class 'dict'>\n",
            "ğŸ“ Root object length: 3\n",
            "ğŸ“ Dictionary with 3 keys:\n",
            "   ğŸ”‘ Logistic Regression: <class 'dict'> (length: 6)\n",
            "   ğŸ”‘ Random Forest: <class 'dict'> (length: 6)\n",
            "   ğŸ”‘ SVC: <class 'dict'> (length: 6)\n",
            "\n",
            "================================================================================\n",
            "ğŸ” Analyzing: ../results/lsd_closed_hamza/lsd_closed_Hamza.pkl\n",
            "============================================================\n",
            "âœ… Successfully loaded pickle file\n",
            "ğŸ“Š Root object type: <class 'dict'>\n",
            "ğŸ“ Root object length: 1\n",
            "ğŸ“ Dictionary with 1 keys:\n",
            "   ğŸ”‘ classification_baseline: <class 'dict'> (length: 271)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import os\n",
        "\n",
        "def load_and_analyze_pickle(file_path):\n",
        "    \"\"\"Load and analyze a pickle file structure\"\"\"\n",
        "    print(f\"ğŸ” Analyzing: {file_path}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"âŒ File not found: {file_path}\")\n",
        "        return None\n",
        "    \n",
        "    # Load the pickle file\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        print(f\"âœ… Successfully loaded pickle file\")\n",
        "        print(f\"ğŸ“Š Root object type: {type(data)}\")\n",
        "        \n",
        "        if hasattr(data, '__len__'):\n",
        "            print(f\"ğŸ“ Root object length: {len(data)}\")\n",
        "        \n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading pickle: {e}\")\n",
        "        return None\n",
        "\n",
        "def explore_dict_structure(data, max_items=10, max_depth=2, current_depth=0):\n",
        "    \"\"\"Explore dictionary structure recursively\"\"\"\n",
        "    indent = \"  \" * current_depth\n",
        "    \n",
        "    if isinstance(data, dict):\n",
        "        print(f\"{indent}ğŸ“ Dictionary with {len(data)} keys:\")\n",
        "        \n",
        "        for i, (key, value) in enumerate(data.items()):\n",
        "            if i >= max_items:\n",
        "                print(f\"{indent}   ... and {len(data) - max_items} more keys\")\n",
        "                break\n",
        "                \n",
        "            print(f\"{indent}   ğŸ”‘ {key}: {type(value)}\", end=\"\")\n",
        "            \n",
        "            if hasattr(value, '__len__') and not isinstance(value, str):\n",
        "                print(f\" (length: {len(value)})\")\n",
        "            else:\n",
        "                print()\n",
        "            \n",
        "            # Show sample values for simple types\n",
        "            if isinstance(value, (str, int, float, bool)) and len(str(value)) < 100:\n",
        "                print(f\"{indent}      ğŸ’¡ Value: {value}\")\n",
        "            \n",
        "            # Recurse for nested structures (limited depth)\n",
        "            if current_depth < max_depth and isinstance(value, dict) and len(value) <= 5:\n",
        "                explore_dict_structure(value, max_items, max_depth, current_depth + 1)\n",
        "    \n",
        "    elif isinstance(data, list):\n",
        "        print(f\"{indent}ğŸ“‹ List with {len(data)} items\")\n",
        "        if len(data) > 0:\n",
        "            print(f\"{indent}   First item type: {type(data[0])}\")\n",
        "            if len(data) <= 5:\n",
        "                for i, item in enumerate(data):\n",
        "                    print(f\"{indent}   [{i}]: {type(item)}\")\n",
        "    else:\n",
        "        print(f\"{indent}ğŸ“„ Single object: {type(data)}\")\n",
        "\n",
        "# Load both pickle files\n",
        "files_to_analyze = [\n",
        "    '../results/lsd_closed_hamza/lsd_closed_hamza_binary_baseline_rs42.pkl',\n",
        "    '../results/lsd_closed_hamza/lsd_closed_Hamza.pkl'\n",
        "]\n",
        "\n",
        "results = {}\n",
        "for file_path in files_to_analyze:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    results[file_name] = load_and_analyze_pickle(file_path)\n",
        "    if results[file_name] is not None:\n",
        "        explore_dict_structure(results[file_name], max_items=5, max_depth=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed analysis of the binary baseline file (smaller file)\n",
        "print(\"ğŸ”¬ DETAILED ANALYSIS: Binary Baseline File\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "binary_file = 'lsd_closed_hamza_binary_baseline_rs42.pkl'\n",
        "if binary_file in results and results[binary_file] is not None:\n",
        "    data = results[binary_file]\n",
        "    \n",
        "    print(f\"ğŸ“‹ Found {len(data)} models:\")\n",
        "    for model_name, model_data in data.items():\n",
        "        print(f\"\\nğŸ¤– Model: {model_name}\")\n",
        "        print(f\"   ğŸ“Š Data keys: {list(model_data.keys())}\")\n",
        "        \n",
        "        # Analyze metric scores\n",
        "        if 'metric_scores' in model_data:\n",
        "            metrics = model_data['metric_scores']\n",
        "            print(f\"   ğŸ“ˆ Metrics available: {list(metrics.keys())}\")\n",
        "            \n",
        "            # Show sample metric scores\n",
        "            for metric_name, metric_data in metrics.items():\n",
        "                if isinstance(metric_data, dict):\n",
        "                    print(f\"      {metric_name}: {list(metric_data.keys())}\")\n",
        "                    # Show actual values if they're simple\n",
        "                    for sub_key, sub_value in metric_data.items():\n",
        "                        if isinstance(sub_value, (int, float, np.number)):\n",
        "                            print(f\"         {sub_key}: {sub_value:.4f}\")\n",
        "        \n",
        "        # Analyze feature importances\n",
        "        if 'feature_importances' in model_data:\n",
        "            fi = model_data['feature_importances']\n",
        "            if isinstance(fi, dict):\n",
        "                print(f\"   ğŸ¯ Feature importance keys: {list(fi.keys())}\")\n",
        "                print(f\"      Number of features: {len(fi)}\")\n",
        "        \n",
        "        # Analyze predictions\n",
        "        if 'predictions' in model_data:\n",
        "            pred = model_data['predictions']\n",
        "            if isinstance(pred, dict):\n",
        "                print(f\"   ğŸ² Prediction keys: {list(pred.keys())}\")\n",
        "else:\n",
        "    print(\"âŒ Binary baseline file not loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed analysis of the large Hamza file\n",
        "print(\"ğŸ”¬ DETAILED ANALYSIS: Large Hamza File\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "hamza_file = 'lsd_closed_Hamza.pkl'\n",
        "if hamza_file in results and results[hamza_file] is not None:\n",
        "    data = results[hamza_file]\n",
        "    \n",
        "    print(f\"ğŸ“‹ Root keys: {list(data.keys())}\")\n",
        "    \n",
        "    # Analyze the classification_baseline structure\n",
        "    if 'classification_baseline' in data:\n",
        "        baseline_data = data['classification_baseline']\n",
        "        print(f\"\\nğŸ“Š Classification baseline contains {len(baseline_data)} subjects\")\n",
        "        \n",
        "        # Show first few subject IDs\n",
        "        subject_ids = list(baseline_data.keys())[:10]\n",
        "        print(f\"ğŸ·ï¸  First 10 subject IDs: {subject_ids}\")\n",
        "        \n",
        "        # Analyze structure of first subject\n",
        "        if len(baseline_data) > 0:\n",
        "            first_subject_id = list(baseline_data.keys())[0]\n",
        "            first_subject_data = baseline_data[first_subject_id]\n",
        "            \n",
        "            print(f\"\\nğŸ” Structure of first subject ({first_subject_id}):\")\n",
        "            print(f\"   Type: {type(first_subject_data)}\")\n",
        "            \n",
        "            if hasattr(first_subject_data, '__len__'):\n",
        "                print(f\"   Length: {len(first_subject_data)}\")\n",
        "            \n",
        "            # If it's a dict, show its keys\n",
        "            if isinstance(first_subject_data, dict):\n",
        "                print(f\"   Keys: {list(first_subject_data.keys())}\")\n",
        "                \n",
        "                # Show sample values for each key\n",
        "                for key, value in first_subject_data.items():\n",
        "                    print(f\"      {key}: {type(value)}\", end=\"\")\n",
        "                    if hasattr(value, '__len__') and not isinstance(value, str):\n",
        "                        print(f\" (length: {len(value)})\")\n",
        "                    else:\n",
        "                        print()\n",
        "                    \n",
        "                    # Show actual values for simple types\n",
        "                    if isinstance(value, (str, int, float, bool)) and len(str(value)) < 100:\n",
        "                        print(f\"         Value: {value}\")\n",
        "                    elif isinstance(value, (list, np.ndarray)) and len(value) <= 10:\n",
        "                        print(f\"         Sample values: {value}\")\n",
        "        \n",
        "        # Analyze patterns across subjects\n",
        "        print(f\"\\nğŸ“ˆ PATTERN ANALYSIS across all {len(baseline_data)} subjects:\")\n",
        "        \n",
        "        # Collect all unique keys from all subjects\n",
        "        all_keys = set()\n",
        "        for subject_data in baseline_data.values():\n",
        "            if isinstance(subject_data, dict):\n",
        "                all_keys.update(subject_data.keys())\n",
        "        \n",
        "        if all_keys:\n",
        "            print(f\"   ğŸ”‘ Unique keys found across all subjects: {sorted(all_keys)}\")\n",
        "            \n",
        "            # Check consistency - do all subjects have the same keys?\n",
        "            key_counts = {}\n",
        "            for subject_data in baseline_data.values():\n",
        "                if isinstance(subject_data, dict):\n",
        "                    for key in subject_data.keys():\n",
        "                        key_counts[key] = key_counts.get(key, 0) + 1\n",
        "            \n",
        "            print(f\"   ğŸ“Š Key frequency across subjects:\")\n",
        "            for key, count in sorted(key_counts.items()):\n",
        "                percentage = (count / len(baseline_data)) * 100\n",
        "                print(f\"      {key}: {count}/{len(baseline_data)} subjects ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"âŒ Large Hamza file not loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary and comparison\n",
        "print(\"ğŸ“Š SUMMARY AND COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# File size comparison\n",
        "import os\n",
        "files = [\n",
        "    '../results/lsd_closed_hamza/lsd_closed_hamza_binary_baseline_rs42.pkl',\n",
        "    '../results/lsd_closed_hamza/lsd_closed_Hamza.pkl'\n",
        "]\n",
        "\n",
        "for file_path in files:\n",
        "    if os.path.exists(file_path):\n",
        "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "        print(f\"ğŸ“ {os.path.basename(file_path)}: {size_mb:.2f} MB\")\n",
        "\n",
        "print(\"\\nğŸ” STRUCTURE COMPARISON:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "binary_file = 'lsd_closed_hamza_binary_baseline_rs42.pkl'\n",
        "hamza_file = 'lsd_closed_Hamza.pkl'\n",
        "\n",
        "if binary_file in results and results[binary_file] is not None:\n",
        "    binary_data = results[binary_file]\n",
        "    print(f\"Binary Baseline File:\")\n",
        "    print(f\"  â€¢ Structure: Dictionary with {len(binary_data)} ML models\")\n",
        "    print(f\"  â€¢ Models: {list(binary_data.keys())}\")\n",
        "    print(f\"  â€¢ Each model contains: {list(next(iter(binary_data.values())).keys())}\")\n",
        "\n",
        "if hamza_file in results and results[hamza_file] is not None:\n",
        "    hamza_data = results[hamza_file]\n",
        "    print(f\"\\nLarge Hamza File:\")\n",
        "    print(f\"  â€¢ Structure: Dictionary with {len(hamza_data)} top-level key(s)\")\n",
        "    if 'classification_baseline' in hamza_data:\n",
        "        baseline_data = hamza_data['classification_baseline']\n",
        "        print(f\"  â€¢ Contains: {len(baseline_data)} subject records\")\n",
        "        print(f\"  â€¢ Subject ID pattern: {list(baseline_data.keys())[:3]}... (showing first 3)\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ KEY INSIGHTS:\")\n",
        "print(f\"  â€¢ Binary baseline file: Contains ML model results (3 models with metrics)\")\n",
        "print(f\"  â€¢ Large Hamza file: Contains individual subject data (271 subjects)\")\n",
        "print(f\"  â€¢ These appear to be different types of ML analysis results\")\n",
        "print(f\"  â€¢ Binary baseline: Model comparison results\")\n",
        "print(f\"  â€¢ Large Hamza: Subject-level analysis results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions for further exploration\n",
        "print(\"ğŸ› ï¸  UTILITY FUNCTIONS FOR FURTHER EXPLORATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def get_model_metrics(model_name, file_key='lsd_closed_hamza_binary_baseline_rs42.pkl'):\n",
        "    \"\"\"Extract metrics for a specific model from binary baseline file\"\"\"\n",
        "    if file_key in results and results[file_key] is not None:\n",
        "        data = results[file_key]\n",
        "        if model_name in data and 'metric_scores' in data[model_name]:\n",
        "            return data[model_name]['metric_scores']\n",
        "    return None\n",
        "\n",
        "def get_subject_data(subject_id, file_key='lsd_closed_Hamza.pkl'):\n",
        "    \"\"\"Extract data for a specific subject from large Hamza file\"\"\"\n",
        "    if file_key in results and results[file_key] is not None:\n",
        "        data = results[file_key]\n",
        "        if 'classification_baseline' in data:\n",
        "            baseline_data = data['classification_baseline']\n",
        "            if subject_id in baseline_data:\n",
        "                return baseline_data[subject_id]\n",
        "    return None\n",
        "\n",
        "def list_available_subjects(file_key='lsd_closed_Hamza.pkl'):\n",
        "    \"\"\"List all available subject IDs\"\"\"\n",
        "    if file_key in results and results[file_key] is not None:\n",
        "        data = results[file_key]\n",
        "        if 'classification_baseline' in data:\n",
        "            return list(data['classification_baseline'].keys())\n",
        "    return []\n",
        "\n",
        "def get_feature_importance(model_name, file_key='lsd_closed_hamza_binary_baseline_rs42.pkl'):\n",
        "    \"\"\"Get feature importance for a specific model\"\"\"\n",
        "    if file_key in results and results[file_key] is not None:\n",
        "        data = results[file_key]\n",
        "        if model_name in data and 'feature_importances' in data[model_name]:\n",
        "            return data[model_name]['feature_importances']\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "print(\"ğŸ¯ EXAMPLE USAGE:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Show available models\n",
        "if 'lsd_closed_hamza_binary_baseline_rs42.pkl' in results:\n",
        "    models = list(results['lsd_closed_hamza_binary_baseline_rs42.pkl'].keys())\n",
        "    print(f\"Available models: {models}\")\n",
        "    \n",
        "    # Get metrics for first model\n",
        "    if models:\n",
        "        first_model_metrics = get_model_metrics(models[0])\n",
        "        print(f\"\\nMetrics for {models[0]}:\")\n",
        "        if first_model_metrics:\n",
        "            pprint(first_model_metrics)\n",
        "\n",
        "# Show sample subjects\n",
        "subjects = list_available_subjects()\n",
        "if subjects:\n",
        "    print(f\"\\nTotal subjects available: {len(subjects)}\")\n",
        "    print(f\"Sample subject IDs: {subjects[:5]}\")\n",
        "    \n",
        "    # Get data for first subject\n",
        "    first_subject_data = get_subject_data(subjects[0])\n",
        "    print(f\"\\nData for {subjects[0]}:\")\n",
        "    if first_subject_data:\n",
        "        if isinstance(first_subject_data, dict):\n",
        "            print(f\"Keys: {list(first_subject_data.keys())}\")\n",
        "        else:\n",
        "            print(f\"Type: {type(first_subject_data)}\")\n",
        "\n",
        "print(f\"\\nâœ… Structure analysis complete!\")\n",
        "print(f\"ğŸ’¡ You can now use the utility functions above to explore specific models or subjects.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "coco",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
