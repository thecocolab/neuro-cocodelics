import argparse
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import mne
import scipy
import numpy as np
import os
import re
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
import warnings
warnings.filterwarnings('ignore')

# --- GLOBAL RC SETTINGS FOR PUBLISHING ---
mpl.rcParams.update({
    "font.family": "DejaVu Sans",
    "font.size": 14,
    "axes.titlesize": 16,
    "axes.labelsize": 14,
    "xtick.labelsize": 12,
    "ytick.labelsize": 12,
    "legend.fontsize": 12,
    "figure.dpi": 300,          # high-res
    "figure.facecolor": "white",
    "axes.facecolor": "white",
    "savefig.facecolor": "white",
    "savefig.bbox": "tight",
    "savefig.pad_inches": 0.1,
})

# Set publication-quality style and color-blind friendly palette
plt.style.use('seaborn-v0_8-whitegrid')
palette = sns.color_palette("colorblind")
sns.set_palette(palette)

def debug_results_structure(results_data, analysis_id="Unknown"):
    """
    Debug function to inspect the structure of results data.
    """
    print(f"\n=== DEBUGGING RESULTS STRUCTURE FOR {analysis_id} ===")
    print(f"Results data type: {type(results_data)}")
    
    if isinstance(results_data, dict):
        print(f"Top-level keys: {list(results_data.keys())}")
        
        for key, value in results_data.items():
            print(f"\n--- Model/Key: {key} ---")
            print(f"Value type: {type(value)}")
            
            if isinstance(value, dict):
                print(f"Sub-keys: {list(value.keys())}")
                
                # Check specific keys we're interested in
                for subkey in ['metric_scores', 'feature_importances', 'predictions', 'selected_features']:
                    if subkey in value:
                        print(f"  {subkey}: {type(value[subkey])}")
                        if isinstance(value[subkey], dict):
                            print(f"    Keys: {list(value[subkey].keys())}")
                        elif hasattr(value[subkey], 'shape'):
                            print(f"    Shape: {value[subkey].shape}")
    print("=" * 60)


def plot_model_performance(results_data, analysis_id, output_dir):
    """
    Plots model performance metrics for a single analysis with publication quality.
    """
    plot_data = []
    cv_data = []  # For cross-validation score distributions
    
    for model_name, model_results in results_data.items():
        if isinstance(model_results, dict) and 'metric_scores' in model_results:
            for metric, scores in model_results['metric_scores'].items():
                # Main performance plot data
                plot_data.append({
                    'analysis_id': analysis_id,
                    'model': model_name,
                    'metric': metric,
                    'score': scores['mean'],
                    'std': scores['std']
                })
                
                # CV fold scores for distribution plots
                if 'fold_scores' in scores:
                    for fold_idx, fold_score in enumerate(scores['fold_scores']):
                        cv_data.append({
                            'model': model_name,
                            'metric': metric,
                            'fold': fold_idx,
                            'score': fold_score
                        })

    if not plot_data:
        print("No metric data found in the results.")
        return

    df_plot = pd.DataFrame(plot_data)
    df_cv = pd.DataFrame(cv_data)

    # 1. Main performance comparison plot
    n_metrics = len(df_plot['metric'].unique())
    fig, axes = plt.subplots(1, n_metrics, figsize=(6*n_metrics, 6))
    if n_metrics == 1:
        axes = [axes]
    
    for i, metric in enumerate(df_plot['metric'].unique()):
        metric_data = df_plot[df_plot['metric'] == metric]
        
        ax = axes[i]
        
        # Use color-blind friendly colors
        bars = ax.bar(range(len(metric_data)), metric_data['score'], 
                     yerr=metric_data['std'], capsize=6, alpha=0.8,
                     color=[palette[j] for j in range(len(metric_data))],
                     edgecolor='black', linewidth=0.5)
        
        # Add value labels on bars with better positioning
        for j, (bar, score, std) in enumerate(zip(bars, metric_data['score'], metric_data['std'])):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + std + 0.02,
                   f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
        
        ax.set_title(f'{metric.upper()}', fontsize=18, fontweight='bold', pad=20)
        ax.set_ylabel('Score', fontsize=16, fontweight='bold')
        ax.set_xlabel('Model', fontsize=16, fontweight='bold')
        ax.set_xticks(range(len(metric_data)))
        ax.set_xticklabels(metric_data['model'], rotation=30, ha='right')
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_ylim(0, min(1.0, metric_data['score'].max() + metric_data['std'].max() + 0.1))
    
    plt.suptitle(f'Model Performance Comparison', fontsize=20, fontweight='bold', y=0.98)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    
    figure_path = os.path.join(output_dir, f"{analysis_id}_model_performance.png")
    plt.savefig(figure_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"Performance plot saved to {figure_path}")
    plt.close()

    # 2. Cross-validation score distributions with improved styling
    if not df_cv.empty:
        n_metrics = len(df_cv['metric'].unique())
        fig, axes = plt.subplots(1, n_metrics, figsize=(6*n_metrics, 6))
        if n_metrics == 1:
            axes = [axes]
        
        # Determine global y-axis limits for consistency
        global_min = df_cv['score'].min() - 0.05
        global_max = df_cv['score'].max() + 0.05
        
        for i, metric in enumerate(df_cv['metric'].unique()):
            metric_cv_data = df_cv[df_cv['metric'] == metric]
            
            ax = axes[i]
            
            # Use different colors for different models
            sns.boxplot(data=metric_cv_data, x='model', y='score', ax=ax, 
                       palette=palette[:len(metric_cv_data['model'].unique())],
                       width=0.6)
            sns.stripplot(data=metric_cv_data, x='model', y='score', ax=ax, 
                         color='darkred', alpha=0.8, size=6, jitter=0.2,
                         edgecolor='white', linewidth=0.5)
            
            ax.set_title(f'CV Score Distribution - {metric.upper()}', fontsize=18, fontweight='bold', pad=20)
            ax.set_ylabel('Score', fontsize=16, fontweight='bold')
            ax.set_xlabel('Model', fontsize=16, fontweight='bold')
            ax.tick_params(axis='x', rotation=0)
            ax.grid(False, axis='y')
            ax.set_ylim(global_min, global_max)
        
        plt.suptitle(f'Cross-Validation Score Distributions', fontsize=20, fontweight='bold', y=0.98)
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        
        cv_figure_path = os.path.join(output_dir, f"{analysis_id}_cv_distributions.png")
        plt.savefig(cv_figure_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"CV distributions plot saved to {cv_figure_path}")
        plt.close()


def plot_feature_importance(results_data, analysis_id, output_dir):
    """
    Plots feature importances for each model with enhanced styling.
    """
    for model_name, model_results in results_data.items():
        if isinstance(model_results, dict) and 'feature_importances' in model_results:
            importances = model_results['feature_importances']
            
            if not importances:
                print(f"No feature importances for {model_name}")
                continue
            
            # Try different possible structures for feature importances
            feature_names = None
            importance_values = None
            
            if isinstance(importances, dict):
                if 'mean' in importances and 'feature_names' in importances:
                    feature_names = importances['feature_names']
                    importance_values = importances['mean']
                elif 'weighted_mean' in importances and 'feature_names' in importances:
                    feature_names = importances['feature_names']
                    importance_values = importances['weighted_mean']
                else:
                    print(f"Feature importance format not recognized for {model_name}:")
                    print(f"Available keys: {list(importances.keys())}")
                    continue
            elif isinstance(importances, np.ndarray):
                importance_values = importances
                # Generate generic feature names if not available
                feature_names = [f"Feature_{i}" for i in range(len(importance_values))]
            else:
                print(f"Unsupported feature importance type: {type(importances)}")
                continue

            if feature_names is None or importance_values is None:
                print(f"Could not extract feature names/values for {model_name}")
                continue

            # Create DataFrame and sort by importance
            df_imp = pd.DataFrame({
                'feature': feature_names,
                'importance': importance_values
            }).sort_values(by='importance', ascending=False).head(20)  # Top 20 features

            if df_imp.empty:
                continue

            plt.figure(figsize=(12, 10))
            
            # Create horizontal bar plot with gradient colors
            y_pos = np.arange(len(df_imp))
            colors = plt.cm.viridis(np.linspace(0, 1, len(df_imp)))
            
            bars = plt.barh(y_pos, df_imp['importance'], alpha=0.8, 
                           color=colors, edgecolor='black', linewidth=0.5)
            
            plt.yticks(y_pos, df_imp['feature'], fontsize=12)
            plt.xlabel('Importance Score', fontsize=16, fontweight='bold')
            plt.ylabel('Features', fontsize=16, fontweight='bold')
            plt.title(f'Top 20 Feature Importances - {model_name}', 
                     fontsize=18, fontweight='bold', pad=20)
            
            # Add value labels on bars
            for i, (bar, importance) in enumerate(zip(bars, df_imp['importance'])):
                plt.text(bar.get_width() + importance*0.01, bar.get_y() + bar.get_height()/2,
                        f'{importance:.4f}', va='center', fontweight='bold', fontsize=10)
            
            plt.grid(True, alpha=0.3, axis='x')
            plt.tight_layout()
            
            figure_path = os.path.join(output_dir, f"{analysis_id}_{model_name}_feature_importance.png")
            plt.savefig(figure_path, dpi=300, bbox_inches='tight', facecolor='white')
            print(f"Feature importance plot saved to {figure_path}")
            plt.close()


def plot_feature_selection_frequency(results_data, analysis_id, output_dir):
    """
    Plots feature selection frequency with enhanced styling.
    """
    for model_name, model_results in results_data.items():
        if isinstance(model_results, dict) and 'feature_frequency' in model_results:
            freq_data = model_results['feature_frequency']
            
            if not freq_data:
                continue
                
            df_freq = pd.DataFrame(list(freq_data.items()), 
                                 columns=['feature', 'frequency']).sort_values(
                                     by='frequency', ascending=False).head(20)
            
            if df_freq.empty:
                continue

            plt.figure(figsize=(12, 10))
            
            # Create horizontal bar plot with distinct colors
            y_pos = np.arange(len(df_freq))
            colors = plt.cm.plasma(np.linspace(0, 1, len(df_freq)))
            
            bars = plt.barh(y_pos, df_freq['frequency'], alpha=0.8,
                           color=colors, edgecolor='black', linewidth=0.5)
            
            plt.yticks(y_pos, df_freq['feature'], fontsize=12)
            plt.xlabel('Selection Frequency (across CV folds)', fontsize=16, fontweight='bold')
            plt.ylabel('Features', fontsize=16, fontweight='bold')
            plt.title(f'Feature Selection Frequency - {model_name}', 
                     fontsize=18, fontweight='bold', pad=20)
            
            # Add frequency labels
            for bar, freq in zip(bars, df_freq['frequency']):
                plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                        f'{freq}', va='center', fontweight='bold', fontsize=11)
            
            plt.grid(True, alpha=0.3, axis='x')
            plt.tight_layout()
            
            figure_path = os.path.join(output_dir, f"{analysis_id}_{model_name}_feature_selection_freq.png")
            plt.savefig(figure_path, dpi=300, bbox_inches='tight', facecolor='white')
            print(f"Feature selection frequency plot saved to {figure_path}")
            plt.close()


def plot_roc_curves(results_data, analysis_id, output_dir):
    """
    Plots ROC curves with publication-quality styling.
    """
    plt.figure(figsize=(10, 8))
    
    for i, (model_name, model_results) in enumerate(results_data.items()):
        if isinstance(model_results, dict) and 'predictions' in model_results:
            predictions = model_results['predictions']
            
            if 'y_true' in predictions and 'y_proba' in predictions:
                y_true = predictions['y_true']
                y_proba = predictions['y_proba']
                
                # Handle binary classification
                if y_proba.ndim == 2 and y_proba.shape[1] == 2:
                    y_proba = y_proba[:, 1]  # Probability of positive class
                elif y_proba.ndim == 1:
                    pass  # Already 1D
                else:
                    print(f"Skipping ROC for {model_name}: unsupported probability shape {y_proba.shape}")
                    continue
                
                try:
                    fpr, tpr, _ = roc_curve(y_true, y_proba)
                    roc_auc = auc(fpr, tpr)
                    
                    # Use thicker lines and distinct colors
                    plt.plot(fpr, tpr, linewidth=3, color=palette[i % len(palette)],
                            label=f'{model_name} (AUC = {roc_auc:.3f})',
                            marker='o', markersize=4, markevery=0.1, alpha=0.8)
                except Exception as e:
                    print(f"Error plotting ROC for {model_name}: {e}")
                    continue
    
    # Random classifier line
    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.7, label='Random Classifier')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=16, fontweight='bold')
    plt.ylabel('True Positive Rate', fontsize=16, fontweight='bold')
    plt.title('ROC Curves', fontsize=18, fontweight='bold', pad=20)
    plt.legend(loc="lower right", fontsize=13, frameon=True, fancybox=True, shadow=True)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    figure_path = os.path.join(output_dir, f"{analysis_id}_roc_curves.png")
    plt.savefig(figure_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"ROC curves plot saved to {figure_path}")
    plt.close()


def plot_confusion_matrices(results_data, analysis_id, output_dir):
    """
    Plots confusion matrices with shared colorbar and improved styling.
    """
    n_models = len(results_data)
    if n_models == 0:
        return
        
    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))
    if n_models == 1:
        axes = [axes]
    
    # Shared colorbar settings
    vmin, vmax = 0, 1
    
    for idx, (model_name, model_results) in enumerate(results_data.items()):
        if isinstance(model_results, dict) and 'predictions' in model_results:
            predictions = model_results['predictions']
            
            if 'y_true' in predictions and 'y_pred' in predictions:
                y_true = predictions['y_true']
                y_pred = predictions['y_pred']
                
                cm = confusion_matrix(y_true, y_pred)
                
                # Normalize confusion matrix
                cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
                
                ax = axes[idx]
                im = ax.imshow(cm_normalized, interpolation='nearest', cmap='Blues',
                              vmin=vmin, vmax=vmax, aspect='equal')
                ax.set_title(f'{model_name}', fontsize=16, fontweight='bold', pad=15)
                
                # Remove background grid and spines for a cleaner heatmap
                ax.grid(False)
                for spine in ax.spines.values():
                    spine.set_visible(False)
                
                # Add text annotations with better contrast
                thresh = cm_normalized.max() / 2.
                for i in range(cm.shape[0]):
                    for j in range(cm.shape[1]):
                        text_color = "white" if cm_normalized[i, j] > thresh else "black"
                        ax.text(j, i, f'{cm[i, j]}\n({cm_normalized[i, j]:.2f})',
                               ha="center", va="center", fontweight='bold', fontsize=12,
                               color=text_color)
                
                ax.set_ylabel('True Label', fontsize=14, fontweight='bold')
                ax.set_xlabel('Predicted Label', fontsize=14, fontweight='bold')
                
                # Set tick labels
                classes = np.unique(np.concatenate([y_true, y_pred]))
                ax.set_xticks(range(len(classes)))
                ax.set_yticks(range(len(classes)))
                ax.set_xticklabels(classes, fontsize=12)
                ax.set_yticklabels(classes, fontsize=12)
    
    # Add shared colorbar to the right of all subplots
    fig.tight_layout(rect=[0,0,0.88,1])  # make room on the right
    cbar_ax = fig.add_axes([0.9, 0.15, 0.02, 0.7])
    cbar = fig.colorbar(im, cax=cbar_ax)
    cbar.set_label("Row-normalized proportion", fontsize=14, fontweight='bold')
    cbar.ax.tick_params(labelsize=12)
    
    plt.suptitle('Confusion Matrices', fontsize=18, fontweight='bold', y=1.02)
    plt.tight_layout(rect=[0, 0, 0.95, 0.95])
    
    figure_path = os.path.join(output_dir, f"{analysis_id}_confusion_matrices.png")
    plt.savefig(figure_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"Confusion matrices plot saved to {figure_path}")
    plt.close()


def generate_summary_report(results_data, analysis_id, output_dir):
    """
    Generate a text summary report of the analysis.
    """
    report_lines = []
    report_lines.append(f"ML Analysis Summary Report: {analysis_id}")
    report_lines.append("=" * 60)
    report_lines.append(f"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")
    
    # Model performance summary
    report_lines.append("MODEL PERFORMANCE SUMMARY:")
    report_lines.append("-" * 30)
    
    for model_name, model_results in results_data.items():
        if isinstance(model_results, dict) and 'metric_scores' in model_results:
            report_lines.append(f"\n{model_name}:")
            for metric, scores in model_results['metric_scores'].items():
                report_lines.append(f"  {metric}: {scores['mean']:.4f} ± {scores['std']:.4f}")
    
    # Feature selection summary (if available)
    report_lines.append("\n\nFEATURE SELECTION SUMMARY:")
    report_lines.append("-" * 30)
    
    for model_name, model_results in results_data.items():
        if isinstance(model_results, dict) and 'selected_features' in model_results:
            selected = model_results['selected_features']
            report_lines.append(f"\n{model_name}:")
            report_lines.append(f"  Number of features selected: {len(selected)}")
            report_lines.append(f"  Selected features: {selected[:10]}{'...' if len(selected) > 10 else ''}")
    
    report_lines.append("\n" + "=" * 60)
    
    # Save report
    report_path = os.path.join(output_dir, f"{analysis_id}_summary_report.txt")
    with open(report_path, 'w') as f:
        f.write('\n'.join(report_lines))
    
    print(f"Summary report saved to {report_path}")

def plot_topo_maps(csv_file, feature, output_dir):
    """Visualizes the specified feature from a CSV file as topographic maps using MNE-Python.
    This function reads a CSV file containing MEG features, filters the data based on the specified feature,
    and generates topographic maps for the LSD and PLA conditions using paired t-tests and permutation t-tests.
    

    Args:
        feature (str): Feature to visualize, e.g., 'higuchiFd', 'detrendedFluctuationAnalysis' (DO NOT ADD MeanEpochs).
        csv_file (str): Name of the CSV file containing the data.
        e.g., 'LSD-PLA-Features.csv'
        output_dir (str): Directory where the output images will be saved.
        
    Returns:
        None: Displays the topographic maps of the specified feature.
    Raises:
        ValueError: If the feature is not found in the CSV file.
    """
    
    
    data = pd.read_csv(f'{output_dir}/{csv_file}')
    feature_prefix = f'feature-{feature}MeanEpochs'
    columns = data.columns
    
    ######################################### THE BLOCK CHECKS IF THE FEATURE IS PRESENT IN THE DATA
    features_all = list()
    for col in data.columns:
        if col == 'subject' or col == 'target':
            continue
        features_all.append(col.split('.')[0].split("-")[1])
    features_all = np.unique(features_all)
    
    if feature_prefix.split('-')[1] not in features_all:
        print(f"Feature prefix '{feature_prefix}' not found in columns.")
        print("Available features (ADD WITHOUT MeanEpochs) :", features_all)
        return
    #########################################
    
    
    #########################################
    filtered_columns = [col for col in columns if col.startswith(feature_prefix)]
    filtered_df = data[['target'] + filtered_columns]
    #########################################

    LSD = filtered_df[filtered_df['target']==1]
    PLA = filtered_df[filtered_df['target']==0]
    LSD = LSD.drop(columns=['target'])
    PLA = PLA.drop(columns=['target'])

    array1 = LSD.values
    array2 = PLA.values

    info = mne.io.read_raw_fif(f'{output_dir}/info_for_plot_topomap_function.fif', preload=True).info

    statistics_method = ['paired_t_test', 'permutation_t_test']
    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 3))

    for ax, (i, stats) in zip(axes, enumerate(statistics_method)):

        if stats == 'paired_t_test':
            t_stats, p_vals = scipy.stats.ttest_rel(array1, array2, axis=0)
            data_to_plot = t_stats * (p_vals < 0.05)
            vmax = np.max(np.abs(data_to_plot))
            vmin = -vmax
                
        if stats == 'permutation_t_test':
            t_stats, p_vals, _ =  mne.stats.permutation_t_test(
                array1 - array2, n_permutations=10000, tail=0, n_jobs=1
            )
            data_to_plot = t_stats * (p_vals < 0.05) 
            vmax = np.max(np.abs(data_to_plot))
            vmin = -vmax
    
        im, _ = mne.viz.plot_topomap(
        data_to_plot,  info, show=False, vlim=(vmin, vmax), cmap="seismic", axes=ax, contours=0, sphere=(0, 0, -0.075, 0.1))
        ax.set_title(f'{csv_file} {feature} {stats}', fontsize=16)
    
    fig.tight_layout(rect=[0, 0.12, 1, 1])

    cbar_ax = fig.add_axes([0.3, 0.04, 0.4, 0.025])  # [left, bottom, width, height]
    fig.colorbar(im, cax=cbar_ax, orientation='horizontal')
    dir_path = f'{output_dir}/Visualization/{csv_file.split(".")[0]}'

    os.makedirs(dir_path, exist_ok=True)

    fig.savefig(f'{dir_path}/{feature}.png', bbox_inches="tight", dpi=300)
    
def main():
    parser = argparse.ArgumentParser(description="Comprehensive ML experiment results visualization.")
    parser.add_argument(
        "--results", "-r", 
        required=True, 
        help="Path to a results pickle file."
    )
    parser.add_argument(
        "--aggregated",
        action="store_true",
        help="Set this flag if the results file is an aggregated pickle with multiple analyses."
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug mode to inspect data structure."
    )
    parser.add_argument(
        "--plots",
        nargs='+',
        default=['all'],
        choices=['performance', 'cv_dist', 'feature_importance', 'feature_selection', 'roc', 'confusion', 'all'],
        help="Specify which plots to generate (default: all)"
    )
    
    args = parser.parse_args()
    
    try:
        results_data = pd.read_pickle(args.results)
    except FileNotFoundError:
        print(f"Error: Results file not found at {args.results}")
        return
    except Exception as e:
        print(f"Error loading results file: {e}")
        return

    output_dir = os.path.dirname(args.results)
    
    def run_visualization(analysis_results, analysis_id):
        """Run visualization for a single analysis."""
        if args.debug:
            debug_results_structure(analysis_results, analysis_id)
        
        plot_functions = {
            'performance': plot_model_performance,
            'feature_importance': plot_feature_importance,
            'feature_selection': plot_feature_selection_frequency,
            'roc': plot_roc_curves,
            'confusion': plot_confusion_matrices,
        }
        
        plots_to_run = args.plots if 'all' not in args.plots else list(plot_functions.keys())
        
        for plot_type in plots_to_run:
            if plot_type in plot_functions:
                try:
                    plot_functions[plot_type](analysis_results, analysis_id, output_dir)
                except Exception as e:
                    print(f"Error generating {plot_type} plot: {e}")
        
        # Always generate summary report
        try:
            generate_summary_report(analysis_results, analysis_id, output_dir)
        except Exception as e:
            print(f"Error generating summary report: {e}")
    
    if args.aggregated:
        # Aggregated file with multiple analyses
        for analysis_id, analysis_results in results_data.items():
            print(f"\nProcessing analysis: {analysis_id}")
            run_visualization(analysis_results, analysis_id)
    else:
        # Single analysis file
        filename = os.path.basename(args.results)
        analysis_id = re.sub(r'\.pkl$', '', filename)
        print(f"\nProcessing single analysis: {analysis_id}")
        run_visualization(results_data, analysis_id)
    
    print(f"\nVisualization complete! All figures saved to: {output_dir}")


if __name__ == "__main__":
    main() 
    
    #python machine-learning/visualize_results.py --results results/lsd_closed/lsd_closed_baseline_binary_baseline_rs42.pkl